{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is second part of this LinkedIn scrapping where we are scrapping data from the User profile who made the post which contains the keyword for which we scrapped the post in the first part. The output of this part will be merged with the Output of the first part. The main motive to scrape the data from the User's profile is to fetch its location so we can filter the post based on our location, i.e. Nigeria in our case.\n",
    "\n",
    "### So we will be having posts from LinkedIn based on our keywords which are from Nigeria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required installs (i.e. pip3 install in terminal): pandas, selenium, bs4, and possibly chromedriver(it may come with selenium)\n",
    "#Download Chromedriver from: https://chromedriver.chromium.org/downloads\n",
    "#To see what version to install: Go to chrome --> on top right click three dot icon --> help --> about Google Chrome\n",
    "#Move the chrome driver to (/usr/local/bin) -- open finder -> Command+Shift+G -> search /usr/local/bin -> move from downloads\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "#import caffeine\n",
    "import random\n",
    "import schedule\n",
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "%matplotlib inline\n",
    "#caffeine.on(display=True)\n",
    "\n",
    "page_url = input(\"Enter the Company Linkedin URL: \")\n",
    "#company_name = page_url[page_url.index('%23')+3:page_url.index('=S')]\n",
    "company_name = page_url[33:-1]\n",
    "\n",
    "try:\n",
    "#    f= open(\"{}/{}_credentials.txt\".format(company_name,company_name),\"r\")\n",
    "    f= open(\"credentials.txt\",\"r\")\n",
    "    contents = f.read()\n",
    "    username = contents.replace(\"=\",\",\").split(\",\")[1]\n",
    "    password = contents.replace(\"=\",\",\").split(\",\")[3]\n",
    "    page = int(contents.replace(\"=\",\",\").split(\",\")[5])\n",
    "    \n",
    "except:\n",
    "     if os.path.isdir(company_name) == False:\n",
    "        try:\n",
    "            os.mkdir(company_name)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % company_name)\n",
    "        else:\n",
    "            print (\"Successfully created the directory %s \" % company_name)\n",
    "\n",
    "#        f= open(\"{}/{}_credentials.txt\".format(company_name,company_name),\"w+\")\n",
    "        f= open(\"credentials.txt\",\"w+\")\n",
    "        username = input('Enter your linkedin username: ')\n",
    "        password = input('Enter your linkedin password: ')\n",
    "        page = 0\n",
    "        f.write(\"username={}, password={}, page_index={}, page_url={}\".format(username,password,page,page_url))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get any existing scraped data\n",
    "try:\n",
    "    scraped = pd.read_csv(\"{}/{}_linkedin_backup.csv\".format(company_name,company_name))\n",
    "    liker_names = list(scraped[\"Id\"])\n",
    "    user_gender = list(scraped[\"Gender\"])\n",
    "    liker_locations = list(scraped[\"Location\"])\n",
    "    liker_headlines = list(scraped[\"Headline\"])\n",
    "    user_bios = list(scraped[\"Bio\"])\n",
    "    est_ages = list(scraped[\"Age\"])\n",
    "    influencers = list(scraped[\"Followed Influencers\"])\n",
    "    companies = list(scraped[\"Followed Companies\"])\n",
    "except:\n",
    "    liker_names = []\n",
    "    user_gender = []\n",
    "    liker_locations = []\n",
    "    liker_headlines = []\n",
    "    user_bios = []\n",
    "    est_ages = []\n",
    "    influencers = []\n",
    "    companies = []\n",
    "    pass\n",
    "\n",
    "#Get the Meta Data\n",
    "try:\n",
    "    linkedin_pages = pd.read_csv(\"meta_data.csv\")\n",
    "    interest_pages = list(linkedin_pages[\"Interest Pages\"])\n",
    "    follower_counts = list(linkedin_pages[\"Follower Counts\"])\n",
    "    follow_rate = list(linkedin_pages[\"Follow Rate\"])\n",
    "except:\n",
    "    interest_pages = []\n",
    "    follower_counts = []\n",
    "    follow_rate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access Webriver\n",
    "#browser = webdriver.Chrome('chromedriver')\n",
    "browser = webdriver.Chrome(r'C:\\Users\\Sanchit\\Downloads\\chromedriver_win32\\chromedriver')\n",
    "\n",
    "#Open login page\n",
    "browser.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')\n",
    "\n",
    "#Enter login info:\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "#Note: replace the keys \"username\" and \"password\" with your LinkedIn login info\n",
    "elementID.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Go to webpage\n",
    "browser.get(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll():\n",
    "    #Simulate scrolling to capture all posts\n",
    "    SCROLL_PAUSE_TIME = random.randint(1,3)\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_up():\n",
    "    #Simulate scrolling to capture all posts\n",
    "    SCROLL_PAUSE_TIME = random.randint(1,3)\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        browser.execute_script(\"window.scrollTo(document.body.scrollHeight, 0);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrolls popups\n",
    "def scroll_popup(class_name):\n",
    "    #Simulate scrolling to capture all posts\n",
    "    SCROLL_PAUSE_TIME = 1.5\n",
    "\n",
    "    # Get scroll height\n",
    "    js_code = \"return document.getElementsByClassName('{}')[0].scrollHeight\".format(class_name)\n",
    "    last_height = browser.execute_script(js_code)\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        path = \"//div[@class='{}']\".format(class_name)\n",
    "        browser.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", browser.find_element_by_xpath(path))\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = browser.execute_script(js_code)\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that estimates user age based on earliest school date or earlier work date\n",
    "def est_age():\n",
    "\n",
    "    browser.switch_to.window(browser.window_handles[1])\n",
    "    date = datetime.today()\n",
    "    current_year = date.strftime(\"%Y\")\n",
    "    school_start_year = \"9999\"\n",
    "    work_start_year = \"9999\"\n",
    "\n",
    "    #Get page source\n",
    "    user_profile = browser.page_source\n",
    "    user_profile = bs(user_profile.encode(\"utf-8\"), \"html\")\n",
    "\n",
    "\n",
    "    #Look for earliest university start date\n",
    "    try:\n",
    "        grad_year = user_profile.findAll('p',{\"class\":\"pv-entity__dates t-14 t-black--light t-normal\"})\n",
    "        \n",
    "        if grad_year == []:\n",
    "            browser.execute_script(\"window.scrollTo(0, 1000);\")\n",
    "            user_profile = browser.page_source\n",
    "            user_profile = bs(user_profile.encode(\"utf-8\"), \"html\")\n",
    "            grad_year = user_profile.findAll('p',{\"class\":\"pv-entity__dates t-14 t-black--light t-normal\"})\n",
    "            \n",
    "        \n",
    "        for d in grad_year:\n",
    "            year = d.find('time').text.strip().replace(' ', '')\n",
    "            start_year = re.sub(r'[a-zA-Z]', r'', year)\n",
    "            start_year = start_year[0:4]\n",
    "            if start_year < school_start_year:\n",
    "                        school_start_year = start_year\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    #Look for earlies work date\n",
    "    try:\n",
    "        #Click see more if it's there\n",
    "        try:\n",
    "            browser.find_element_by_xpath(\"//button[@class='pv-profile-section__see-more-inline pv-profile-section__text-truncate-toggle link-without-visited-state']\").click()\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            pass\n",
    "\n",
    "        work_start = user_profile.findAll('h4', {\"class\":\"pv-entity__date-range t-14 t-black--light t-normal\"})\n",
    "\n",
    "\n",
    "        for d in work_start:\n",
    "            start_date = d.find('span',class_=None)\n",
    "            start_date = start_date.text.strip().replace(' ', '')\n",
    "            start_date = re.sub(r'[a-zA-Z]', r'', start_date)\n",
    "            start_year = start_date[0:4]\n",
    "            if start_year < work_start_year:\n",
    "                    work_start_year = start_year\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Compare work and school start dates to avoid adult degress\n",
    "    if school_start_year < work_start_year:\n",
    "        #Estimate age based on avg university start age of 18\n",
    "        est_birth_year = int(school_start_year) - 18\n",
    "        est_age = int(current_year) - est_birth_year\n",
    "\n",
    "    else:\n",
    "        #Estimate age based on avg post college work start date of 22\n",
    "        est_birth_year = int(work_start_year) - 22\n",
    "        est_age = int(current_year) - est_birth_year\n",
    "\n",
    "    if est_age <= 0:\n",
    "        est_age = 'unknown'\n",
    "    \n",
    "    return est_age\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that Scrapes user data\n",
    "def get_user_data(user_profile):\n",
    "    \n",
    "    global skip_count\n",
    "\n",
    "    try:\n",
    "        name = user_profile.find('li',{'class':\"inline t-24 t-black t-normal break-words\"})\n",
    "        name = name.text.strip()\n",
    "    except:\n",
    "        print(\"This is a company. Skipping for now.\")\n",
    "        return None\n",
    "    \n",
    "    #Make sure liker isn't a duplicate\n",
    "    if name not in liker_names:\n",
    "\n",
    "        skip_count = 0\n",
    "        liker_names.append(name)\n",
    "        split_name = name.split(\" \", 2)\n",
    "        #Get Liker Gender\n",
    "        user_gender.append(d.get_gender(split_name[0])+\"^ \")\n",
    "\n",
    "        try:\n",
    "            #Get Liker Location\n",
    "            location = user_profile.find('li',{'class':\"t-16 t-black t-normal inline-block\"})\n",
    "            liker_locations.append(location.text.strip()+\"^ \")\n",
    "        except:\n",
    "            liker_locations.append(\"No Location\")\n",
    "\n",
    "        try:\n",
    "            #Get Liker Headline\n",
    "            headline = user_profile.find('h2',{\"class\":\"mt1 t-18 t-black t-normal break-words\"})\n",
    "            liker_headlines.append(headline.text.strip())\n",
    "        except:\n",
    "            liker_headlines.append(\"No Headline\")\n",
    "\n",
    "\n",
    "        #Get Liker Bio\n",
    "        try:\n",
    "            browser.find_element_by_xpath(\"//a[@id='line-clamp-show-more-button']\").click()\n",
    "            time.sleep(1)\n",
    "            user_profile = browser.page_source\n",
    "            user_profile = bs(user_profile.encode(\"utf-8\"), \"html\")\n",
    "            bio = user_profile.findAll(\"span\",{\"class\":\"lt-line-clamp__raw-line\"})\n",
    "            user_bios.append(bio[0].text.strip())\n",
    "        except:\n",
    "            try:\n",
    "                bio_lines = []\n",
    "                bios = user_profile.findAll('span',{\"class\":\"lt-line-clamp__line\"})\n",
    "                for b in bios:\n",
    "                    bio_lines.append(b.text.strip())\n",
    "                bio = \",\".join(bio_lines).replace(\",\", \". \")\n",
    "                user_bios.append(bio)\n",
    "\n",
    "            except:\n",
    "                user_bios.append('No Bio')\n",
    "                pass\n",
    "\n",
    "        #Get estimated age using our age function\n",
    "        age = est_age()\n",
    "        est_ages.append(age)\n",
    "\n",
    "\n",
    "\n",
    "        #Click see more on user interests\n",
    "        try: \n",
    "            interest_path = \"//a[@data-control-name='view_interest_details']\"\n",
    "            browser.find_element_by_xpath(interest_path).click()\n",
    "        except:\n",
    "            scroll()\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                interest_path = \"//a[@data-control-name='view_interest_details']\"\n",
    "                browser.find_element_by_xpath(interest_path).click()\n",
    "            except:\n",
    "                influencers.append(\"No Influencers^ \")\n",
    "                companies.append(\"No Companies^ \")\n",
    "                return\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        #Scrape the influencers the user follows\n",
    "        try:\n",
    "            influencer_path = \"//a[@id='pv-interests-modal__following-influencers']\"\n",
    "            browser.find_element_by_xpath(influencer_path).click()\n",
    "\n",
    "            #Scroll the end of list\n",
    "            class_name = 'entity-all pv-interests-list ml4 pt2 ember-view'\n",
    "            #interest_box_path = \"//div[@class='entity-all pv-interests-list ml4 pt2 ember-view']\"\n",
    "            scroll_popup(class_name)\n",
    "\n",
    "            influencer_page = browser.page_source\n",
    "            influencer_page = bs(influencer_page.encode(\"utf-8\"), \"html\")\n",
    "            influencer_list = influencer_page.findAll(\"li\",{\"class\":\"entity-list-item\"})\n",
    "\n",
    "\n",
    "            user_influencers = \"\"\n",
    "            for i in influencer_list:\n",
    "                name = i.find(\"span\",{\"class\":\"pv-entity__summary-title-text\"})\n",
    "                name = name.text.strip()\n",
    "                user_influencers += name + \"^ \"\n",
    "                cleaned_name = name.replace(\",\",\"\")\n",
    "                \n",
    "                if cleaned_name not in interest_pages:\n",
    "                    interest_pages.append(cleaned_name)\n",
    "                    follower_count = i.find('p', {\"class\":\"pv-entity__follower-count\"}).text.strip()\n",
    "                    follower_count = follower_count.split(' ')\n",
    "                    follower_count = follower_count[0]\n",
    "                    follower_counts.append(follower_count)\n",
    "                    \n",
    "                    #Calc the follower rate\n",
    "                    total_linkedin_users = 260000000\n",
    "                    follow_percent = float(follower_count.replace(\",\",\"\"))/total_linkedin_users * 100\n",
    "                    follow_rate.append(follow_percent)\n",
    "\n",
    "            influencers.append(user_influencers)\n",
    "\n",
    "\n",
    "        except:\n",
    "            influencers.append(\"No Influencers^ \")\n",
    "\n",
    "\n",
    "\n",
    "        #Scrape the companies the user follows\n",
    "        try:\n",
    "            company_path = \"//a[@id='pv-interests-modal__following-companies']\"\n",
    "            browser.find_element_by_xpath(company_path).click()\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "            #Scroll the end of list\n",
    "            class_name = 'entity-all pv-interests-list ml4 pt2 ember-view'\n",
    "            #interest_box_path = \"//div[@class='entity-all pv-interests-list ml4 pt2 ember-view']\"\n",
    "            scroll_popup(class_name)\n",
    "\n",
    "\n",
    "            company_page = browser.page_source\n",
    "            company_page = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "            company_list = company_page.findAll(\"li\",{\"class\":\"entity-list-item\"})\n",
    "\n",
    "\n",
    "            user_companies = \"\"\n",
    "            for i in company_list:\n",
    "                name = i.find(\"span\",{\"class\":\"pv-entity__summary-title-text\"})\n",
    "                name = name.text.strip()\n",
    "                user_companies += name + \"^ \"\n",
    "                cleaned_name = name.replace(\",\",\"\")\n",
    "                \n",
    "                if cleaned_name not in interest_pages:\n",
    "                    interest_pages.append(cleaned_name)\n",
    "                    follower_count = i.find('p', {\"class\":\"pv-entity__follower-count\"}).text.strip()\n",
    "                    follower_count = follower_count.split(' ')\n",
    "                    follower_count = follower_count[0]\n",
    "                    follower_counts.append(follower_count)\n",
    "                    \n",
    "                    #Calc the follower rate\n",
    "                    total_linkedin_users = 260000000\n",
    "                    follow_percent = float(follower_count.replace(\",\",\"\"))/total_linkedin_users * 100\n",
    "                    follow_rate.append(follow_percent)\n",
    "\n",
    "            companies.append(user_companies)\n",
    "                \n",
    "\n",
    "        except:\n",
    "            companies.append(\"No Companies^ \")\n",
    "\n",
    "    else:\n",
    "        skip_count+=1\n",
    "        time.sleep(random.randint(2,7))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(words):\n",
    "    wordcount = {}\n",
    "    for word in words.split('^ '):\n",
    "        word = word.replace(\"\\\"\",\"\")\n",
    "        word = word.replace(\"!\",\"\")\n",
    "        word = word.replace(\"â€œ\",\"\")\n",
    "        word = word.replace(\"â€˜\",\"\")\n",
    "        word = word.replace(\"*\",\"\")\n",
    "        word = word.replace(\"?\",\"\")\n",
    "        word = word.replace(\"mostly_male\",\"male\")\n",
    "        word = word.replace(\"mostly_female\",\"female\")\n",
    "        \n",
    "        exclude_words = [\"No Influencers\", \"No Companies\", \"unknown\", \"andy\", \"\"]\n",
    "        \n",
    "        if word not in exclude_words:\n",
    "            if word not in wordcount:\n",
    "                wordcount[word] = 1\n",
    "            else:\n",
    "                wordcount[word] += 1\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    return wordcount\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(wc):\n",
    "    \n",
    "    total_scraped = len(user_gender)\n",
    "    \n",
    "    trimmed_count = collections.Counter(wc).most_common(300)\n",
    "\n",
    "    words = []\n",
    "    count = []\n",
    "    percent = []\n",
    "    interest_index = []\n",
    "    interest_diff = []\n",
    "    for item in trimmed_count:\n",
    "        words.append(item[0])\n",
    "        count.append(item[1])\n",
    "        \n",
    "    for c in count:\n",
    "        percent.append(round(((c/total_scraped) * 100), 2))\n",
    "    \n",
    "    #make interest dictionary from meta data\n",
    "    interest_dict = dict(zip(interest_pages, follow_rate))\n",
    "            \n",
    "    n=0\n",
    "    for w in words:\n",
    "        if w in list(interest_dict.keys()):\n",
    "            if float(interest_dict[w]) != 0:\n",
    "                index = float(percent[n])/float(interest_dict[w])\n",
    "                interest_index.append(round(index,2))\n",
    "                interest_diff.append(round(float(percent[n])-float(interest_dict[w]),2))\n",
    "                n+=1\n",
    "            else:\n",
    "                interest_index.append(\"NA\")\n",
    "                interest_diff.append(\"NA\")\n",
    "                n+=1\n",
    "        else:\n",
    "            interest_index.append(\"NA\")\n",
    "            interest_diff.append(\"NA\")\n",
    "            n+=1\n",
    "        \n",
    "\n",
    "    data = {\"Word\": words,\"Count\": count, \"Percentage\": percent, \"Index\":interest_index, \"Absolute Difference\":interest_diff}\n",
    "\n",
    "    df = pd.DataFrame(data, index =None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(interest):\n",
    "    clean_list = []\n",
    "    for item in interest:\n",
    "        clean = item.replace('^','')\n",
    "        clean_list.append(clean.title())\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_interests(interest):\n",
    "    clean_list = []\n",
    "    for item in interest:\n",
    "        clean = item.replace('^',',')\n",
    "        clean_list.append(clean)\n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_interests():\n",
    "    company_list = \",\".join(companies).replace(',','')\n",
    "    company_count = word_counter(company_list)\n",
    "    common_companies = get_df(company_count)\n",
    "\n",
    "    influencer_list = \",\".join(influencers).replace(',','')\n",
    "    influencer_count = word_counter(influencer_list)\n",
    "    common_influencers = get_df(influencer_count)\n",
    "    \n",
    "    gender_list = \",\".join(user_gender).replace(',','')\n",
    "    gender_count = word_counter(gender_list)\n",
    "    common_genders = get_df(gender_count)\n",
    "\n",
    "    location_list = \",\".join(liker_locations).replace(',','')\n",
    "    location_count = word_counter(location_list)\n",
    "    common_locations = get_df(location_count)\n",
    "    \n",
    "    return common_companies, common_influencers, common_genders, common_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interests(df1,df2,df3,df4):\n",
    "    company_plot = df1[0:24].plot.barh(x='Word',y='Percentage')\n",
    "    company_plot.invert_yaxis()\n",
    "    company_plot.set_ylabel('Companies')\n",
    "    company_plot.figure.savefig(\"{}/c_plot.png\".format(company_name), dpi = 100, bbox_inches = \"tight\")\n",
    "\n",
    "    influencer_plot = df2[0:24].plot.barh(x='Word',y='Percentage')\n",
    "    influencer_plot.invert_yaxis()\n",
    "    influencer_plot.set_ylabel('Influencers')\n",
    "    influencer_plot.figure.savefig(\"{}/i_plot.png\".format(company_name), dpi = 100, bbox_inches = \"tight\")\n",
    "    \n",
    "    gender_plot = df3[0:24].plot.barh(x='Word',y='Percentage')\n",
    "    gender_plot.invert_yaxis()\n",
    "    gender_plot.set_ylabel('Gender')\n",
    "    gender_plot.figure.savefig(\"{}/g_plot.png\".format(company_name), dpi = 100, bbox_inches = \"tight\")\n",
    "\n",
    "    location_plot = df4[0:24].plot.barh(x='Word',y='Percentage')\n",
    "    location_plot.invert_yaxis()\n",
    "    location_plot.set_ylabel('Locations')\n",
    "    location_plot.figure.savefig(\"{}/l_plot.png\".format(company_name), dpi = 100, bbox_inches = \"tight\")\n",
    "    \n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df():\n",
    "    #Constructing Pandas Dataframe\n",
    "    data = {\n",
    "        \"Gender\": clean_list(user_gender),\n",
    "        \"Location\": clean_list(liker_locations),\n",
    "        \"Age\": est_ages,\n",
    "        \"Headline\": liker_headlines,\n",
    "        \"Bio\": user_bios,\n",
    "        \"Followed Influencers\": clean_interests(influencers),\n",
    "        \"Followed Companies\": clean_interests(companies)\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    #Make backup data from to save our progress\n",
    "    backup_data = {\n",
    "        \"Id\": liker_names,\n",
    "        \"Gender\": user_gender,\n",
    "        \"Location\": liker_locations,\n",
    "        \"Age\": est_ages,\n",
    "        \"Headline\": liker_headlines,\n",
    "        \"Bio\": user_bios,\n",
    "        \"Followed Influencers\": influencers,\n",
    "        \"Followed Companies\": companies    \n",
    "    }\n",
    "    \n",
    "    backup_df = pd.DataFrame(backup_data)\n",
    "    \n",
    "    \n",
    "    #Make a df of ages stats\n",
    "    age_list = []\n",
    "    for a in df[\"Age\"]:\n",
    "        if a != \"unknown\":\n",
    "            age_list.append(int(a))\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    age_data = {\"Ages\": age_list}    \n",
    "    \n",
    "    ages = pd.DataFrame(age_data)\n",
    "    age_stats = ages.describe()\n",
    "    age_stats = pd.DataFrame(age_stats)\n",
    "    \n",
    "\n",
    "    #Exporting csv to program folder for backup\n",
    "    backup_df.to_csv(\"{}/{}_linkedin_backup.csv\".format(company_name,company_name), encoding='utf-8', index=True)\n",
    "    \n",
    "    #Get data frames of interest counts\n",
    "    common_companies, common_influencers, common_genders, common_locations = count_interests()\n",
    "    \n",
    "    #Plot the interest counts\n",
    "    plot_interests(common_companies, common_influencers, common_genders, common_locations)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    #Create/Update Excel file\n",
    "    writer = pd.ExcelWriter(\"{}/{}_linkedin.xlsx\".format(company_name,company_name), engine='xlsxwriter')\n",
    "    df.to_excel(writer, sheet_name='Group Members', index=False)\n",
    "    common_companies.to_excel(writer, sheet_name='Company Interest', index=False)\n",
    "    common_influencers.to_excel(writer, sheet_name='Influencer Interest', index=False)\n",
    "    age_stats.to_excel(writer, sheet_name='Demographic Stats', index=True)\n",
    "    writer.save()\n",
    "    \n",
    "    wb = load_workbook(\"{}/{}_linkedin.xlsx\".format(company_name,company_name))\n",
    "\n",
    "    #Adding plots to the sheets\n",
    "    cws = wb[\"Company Interest\"]\n",
    "    c_img = openpyxl.drawing.image.Image('{}/c_plot.png'.format(company_name))\n",
    "    c_img.anchor = 'H5'\n",
    "    cws.add_image(c_img)\n",
    "\n",
    "    iws = wb[\"Influencer Interest\"]\n",
    "    i_img = openpyxl.drawing.image.Image('{}/i_plot.png'.format(company_name))\n",
    "    i_img.anchor = 'H5'\n",
    "    iws.add_image(i_img)\n",
    "    \n",
    "    dws = wb[\"Demographic Stats\"]\n",
    "    g_img = openpyxl.drawing.image.Image('{}/g_plot.png'.format(company_name))\n",
    "    g_img.anchor = 'D2'\n",
    "    dws.add_image(g_img)\n",
    "    l_img = openpyxl.drawing.image.Image('{}/l_plot.png'.format(company_name))\n",
    "    l_img.anchor = 'B21'\n",
    "    dws.add_image(l_img)\n",
    "\n",
    "    #Save Excel file\n",
    "    wb.save(\"{}/{}_linkedin.xlsx\".format(company_name,company_name))\n",
    "    \n",
    "    #Keep Track of where we are in the foller list\n",
    "    f= open(\"{}/{}_credentials.txt\".format(company_name,company_name),\"w+\")\n",
    "    f.write(\"username={}, password={}, page_index={}, page_url={}\".format(username,password,page,page_url))\n",
    "    f.close()\n",
    "        \n",
    "    #Export the Meta Data\n",
    "    meta_data = {\n",
    "    \"Interest Pages\": interest_pages,\n",
    "    \"Follower Counts\": follower_counts,\n",
    "    \"Follow Rate\": follow_rate\n",
    "    }\n",
    "\n",
    "    meta_df = pd.DataFrame(meta_data)\n",
    "\n",
    "    meta_df.to_csv(\"meta_data.csv\", encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source():  \n",
    "    company_page = browser.page_source\n",
    "    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "    #linkedin_soup.prettify()\n",
    "\n",
    "    return linkedin_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_users(links):\n",
    "    \n",
    "    for link in links[0:3]:\n",
    "\n",
    "        link.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        #switch to post page and get source\n",
    "        browser.switch_to.window(browser.window_handles[1])\n",
    "        user_profile = get_source()\n",
    "#         post_block = post_source.findAll(\"div\",{\"class\":\"core-rail update-outlet\"})\n",
    "#         user_profile = browser.page_source\n",
    "#         user_profile = bs(user_profile.encode(\"utf-8\"), \"html\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "        try:\n",
    "            #scape the post and then close and switch back to main page\n",
    "            get_user_data(user_profile)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "        browser.close()\n",
    "        browser.switch_to.window(browser.window_handles[0])\n",
    "        time.sleep(random.randint(4,24))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_links():\n",
    "    #scroll to end of list\n",
    "    scroll()\n",
    "\n",
    "    #find all post links\n",
    "    path = \"//span[@class='entity-result__title-line flex-shrink-1 entity-result__title-text--black entity-result__title-text--black']\"\n",
    "    links = browser.find_elements_by_xpath(path)\n",
    "    \n",
    "    scroll_up()\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time():\n",
    "    current_time = datetime.now().strftime(\"%H:%M\")\n",
    "    return current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global page\n",
    "    page_limit = 2\n",
    "    daily_count = 0\n",
    "    skip_count = 0\n",
    "    daily_limit = random.randint(2000,25000)\n",
    "    \n",
    "    while page < page_limit:\n",
    "\n",
    "        links = get_user_links()\n",
    "        scrape_users(links)\n",
    "        export_df()\n",
    "        page += 1\n",
    "        print(\"{} pages scraped so far\".format(page))\n",
    "        \n",
    "        \n",
    "        #Stop if reached daily page view limit\n",
    "        if daily_count >= daily_limit:\n",
    "            print(\"Daily page limit of {} has been reached. Stopping for the day to prevent auto signout.\".format(str(daily_limit)))\n",
    "            while current_time() >= \"01:00\":\n",
    "                schedule.run_pending()\n",
    "                time.sleep(60)\n",
    "            daily_count = 0\n",
    "\n",
    "        #Stop for the night\n",
    "        while current_time() < \"07:05\":\n",
    "            schedule.run_pending()\n",
    "            time.sleep(60)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            browser.find_element_by_xpath('//*[@class=\"artdeco-pagination__button artdeco-pagination__button--next artdeco-button artdeco-button--muted artdeco-button--icon-right artdeco-button--1 artdeco-button--tertiary ember-view\"]').click()\n",
    "            time.sleep(random.randint(1,5))\n",
    "        except:\n",
    "            print(\"All pages scraped\")\n",
    "            break   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a company. Skipping for now.\n",
      "This is a company. Skipping for now.\n",
      "This is a company. Skipping for now.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'esults/content/?keywords=stalk&origin=SWITCH_SEARCH_VERTICA/esults/content/?keywords=stalk&origin=SWITCH_SEARCH_VERTICA_linkedin_backup.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-451876906980>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#browser.switch_to.window(browser.window_handles[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-4495994fa9ae>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_user_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mscrape_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mexport_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mpage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} pages scraped so far\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-9fbdda68a853>\u001b[0m in \u001b[0;36mexport_df\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m#Exporting csv to program folder for backup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mbackup_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}_linkedin_backup.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompany_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m#Get data frames of interest counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         )\n\u001b[1;32m-> 3170\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             )\n\u001b[0;32m    192\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'esults/content/?keywords=stalk&origin=SWITCH_SEARCH_VERTICA/esults/content/?keywords=stalk&origin=SWITCH_SEARCH_VERTICA_linkedin_backup.csv'"
     ]
    }
   ],
   "source": [
    "#browser.switch_to.window(browser.window_handles[0])\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
